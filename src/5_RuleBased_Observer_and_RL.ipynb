{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13236e9b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from PIL import Image\n",
    "from engine import train_one_epoch, evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "import transforms as T\n",
    "import time\n",
    "\n",
    "from scipy import ndimage\n",
    "from skimage.feature import corner_harris, corner_peaks, corner_subpix\n",
    "from skimage.feature import peak_local_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "958ffa12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "68f97158",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.backbone.body.conv1 = nn.Conv2d(36, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da66addb-0614-40fd-b846-1892ed17dc3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class StarDataset(object):\n",
    "    def __init__(self, path, transforms, window_size):\n",
    "        self.root = path\n",
    "        self.transforms = transforms\n",
    "        pth = os.listdir(path)\n",
    "        self.label_sequences = []\n",
    "\n",
    "        self.dir_paths = []\n",
    "        if os.path.isdir(path):\n",
    "            self.dir_paths.append(path + '/')\n",
    "            print(f\"Loaded {path}\")\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.seq_indexs = []\n",
    "        self.seq_indexs.append((0, 0, len(pth)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.seq_indexs[-1][-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        for i, start, end in self.seq_indexs:\n",
    "            if idx >= start and idx < end:\n",
    "                real_idx = idx - start\n",
    "\n",
    "                if real_idx > 3:\n",
    "                    entire_data1 = np.load(self.dir_paths[i] + '/' + str(int(real_idx) - 3) + \".npy\", allow_pickle=True)\n",
    "                    entire_data2 = np.load(self.dir_paths[i] + '/' + str(int(real_idx) - 2) + \".npy\", allow_pickle=True)\n",
    "                    entire_data3 = np.load(self.dir_paths[i] + '/' + str(int(real_idx) - 1) + \".npy\", allow_pickle=True)\n",
    "                    entire_data4 = np.load(self.dir_paths[i] + '/' + str(int(real_idx) + 0) + \".npy\", allow_pickle=True)\n",
    "                else:\n",
    "                    entire_data1 = np.load(self.dir_paths[i] + '/' + str(int(real_idx)) + \".npy\", allow_pickle=True)\n",
    "                    entire_data2 = entire_data1\n",
    "                    entire_data3 = entire_data1\n",
    "                    entire_data4 = entire_data1\n",
    "\n",
    "                data1 = entire_data1[0]\n",
    "                data2 = entire_data2[0]\n",
    "                data3 = entire_data3[0]\n",
    "                data4 = entire_data4[0]\n",
    "                \n",
    "                masks = entire_data4[1]\n",
    "\n",
    "                break\n",
    "\n",
    "        input_data = self.preprocessing(data1, data2, data3, data4)\n",
    "        return input_data, torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
    "\n",
    "    def preprocessing(self, data1, data2, data3, data4):\n",
    "        # 0 ground 1 air 2 building 3 spell 4 ground 5 air 6 building 7 spell 8 resource 9 vision 10 terrain\n",
    "        temp = np.zeros([self.window_size, 9, data1.shape[1], data1.shape[2]])\n",
    "\n",
    "        temp[0] = data1\n",
    "        temp[1] = data2\n",
    "        temp[2] = data3\n",
    "        temp[3] = data4\n",
    "\n",
    "        data = temp\n",
    "        data = data.reshape(self.window_size * data.shape[1], data.shape[2], -1)\n",
    "        # #data = data.reshape(self.window_size*data.shape[0],data.shape[1],-1)\n",
    "        # label = np.array([label[0]/3456, label[1]/3720])\n",
    "        return torch.FloatTensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load(path: str, replay: int):\n",
    "    vpd_paths = glob(os.path.join(path, \"*/\") + \"{}.rep.vpd\".format(str(replay)))\n",
    "    print(vpd_paths)\n",
    "    vpds = [pd.read_csv(_, index_col=None) for _ in vpd_paths]\n",
    "    return vpds\n",
    "\n",
    "replay = 212\n",
    "datset_dir =  \"../data/vpds2/\"\n",
    "a = load(datset_dir,replay)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vpx_array = a[0]['vpx'].to_numpy() # bcm\n",
    "vpy_array = a[0]['vpy'].to_numpy() # bc,"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testing_replay = [\"212\"]\n",
    "datset_dir = \"C:/Users/joo/PycharmProjects/starcraft_replay_reconstructor-devel/starcraft_replay_reconstructor-devel/result/\"\n",
    "\n",
    "for i in testing_replay:    \n",
    "    replay_dir = datset_dir + str(i)\n",
    "    replay_name = int(replay_dir.split('/')[-1])\n",
    "    print(replay_name)\n",
    "    dataset = StarDataset(replay_dir, get_transform(train=False), window_size=4)\n",
    "    \n",
    "    dataset_len = len(dataset)    \n",
    "    Start, End, Step = 0, len(dataset), 1\n",
    "    test_img_array = []\n",
    "    test_img_one_channel_array = []\n",
    "    test_target_array = []\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    for i in range(Start, End, Step):\n",
    "        img_t = dataset[i][0]\n",
    "        target_t = dataset[i][1]\n",
    "        test_img_array.append(img_t)\n",
    "        img_one_channel = img_t.sum(axis=0, keepdim=True)\n",
    "        test_img_one_channel_array.append(img_one_channel)\n",
    "        test_target_array.append(target_t)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "acac1 = test_img_array[3000][:2].sum(axis=0).detach().numpy()    \n",
    "acac1 = np.where(acac1 >= 1, 0.16, 0.0)\n",
    "acac2 = test_img_array[3000][2:3].sum(axis=0).detach().numpy()    \n",
    "acac2 = np.where(acac2 >= 1, 0.32, 0.0)\n",
    "acac3 = test_img_array[3000][3:4].sum(axis=0).detach().numpy()    \n",
    "acac3 = np.where(acac2 >= 1, 0.48, 0.0)\n",
    "\n",
    "acac4 = test_img_array[3000][4:6].sum(axis=0).detach().numpy()    \n",
    "acac4 = np.where(acac4 >= 1, 0.64, 0.0)\n",
    "acac5 = test_img_array[3000][6:7].sum(axis=0).detach().numpy()    \n",
    "acac5 = np.where(acac5 >= 1, 0.8, 0.0)\n",
    "acac6 = test_img_array[3000][7:8].sum(axis=0).detach().numpy()    \n",
    "acac6 = np.where(acac6 >= 1, 0.96, 0.0)\n",
    "\n",
    "viewport_x = int(vpx_array[3000]/32)\n",
    "viewport_y = int(vpy_array[3000]/32)\n",
    "\n",
    "acac = acac1 + acac2 + acac3  + acac4  + acac5  + acac6\n",
    "acac = np.where(acac >= 1, 0.96, acac)\n",
    "\n",
    "acac[viewport_y:viewport_y+1, viewport_x:viewport_x+20]=1\n",
    "acac[viewport_y+12:viewport_y+13, viewport_x:viewport_x+21]=1    \n",
    "acac[viewport_y:viewport_y+12, viewport_x:viewport_x+1]=1\n",
    "acac[viewport_y:viewport_y+12, viewport_x+20:viewport_x+21]=1\n",
    "ax.imshow(acac, cmap=\"RdYlBu_r\") # RdYlBu_r # gist_earth_r\n",
    "\n",
    "plt.show() "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rule-Based "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage.metrics import mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "action = 0\n",
    "\n",
    "class StarObserverEnv2(gym.Env):\n",
    "    def __init__(self, render_mode=None):\n",
    "        super(StarObserverEnv2, self).__init__()        \n",
    "        \n",
    "        #self.reward_range = (0, MAX_ACCOUNT_BALANCE)\n",
    "        N_DISCRETE_ACTIONS = 9        \n",
    "        N_CHANNELS, HEIGHT, WIDTH = 8, 96, 20\n",
    "        \n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        self.current_step = 0\n",
    "        self.current_episode_step = 0\n",
    "        self.current_action = 0\n",
    "        # self.previous_state = torch.zeros(16, 20, 12)\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=\n",
    "                    (N_CHANNELS, HEIGHT, WIDTH), dtype=np.float16)\n",
    "        self.total_angle = []\n",
    "        self.action_ = [\"left,up\", \"up\", \"up,right\", \"left\", \"stop\", \"right\", \"down,left\", \"down\", \"right,down\"]   \n",
    "        self.action_dic = {0: ['-','-'], 1: ['-','0'],2: ['-','+'],3: ['0','-'],4: ['0', '0'],\n",
    "            5: ['0','+'],6: ['+','-'],7: ['+','0'],8: ['+','+']}\n",
    "        self.pre_frame_center_coor = 0,0\n",
    "        self.curr_frame_center_coor = 0,0\n",
    "        \n",
    "    def _next_observation(self):\n",
    "        self.current_step\n",
    "\n",
    "#         viewport_x = int(predictions[self.current_step][0]['boxes'][0][0])\n",
    "#         viewport_y = int(predictions[self.current_step][0]['boxes'][0][1])        \n",
    "#         print(\"vx\",viewport_x)\n",
    "#         print(viewport_y)\n",
    "        \n",
    "        \n",
    "        viewport_x =  21\n",
    "        viewport_y =  25\n",
    "        \n",
    "        if viewport_x == 0 or viewport_y == 0 or viewport_x == (128-20) or viewport_y == (128-12) :\n",
    "            done = 1\n",
    "            return 0,0, done\n",
    "\n",
    "        state1 = []\n",
    "        next_state1 = []\n",
    "        \n",
    "        for i in range(4):        \n",
    "            current_state_channel = test_img_array[self.current_step-3+i][0:8]\n",
    "            current_state_viewport = current_state_channel.clone()[:,viewport_y:viewport_y+12, viewport_x:viewport_x+20]\n",
    "            \n",
    "            next_state_channel = test_img_array[self.current_step+i+1][0:8]\n",
    "            next_state_viewport = next_state_channel.clone()[:,viewport_y:viewport_y+12, viewport_x:viewport_x+20]\n",
    "            \n",
    "            state1.append(current_state_viewport)\n",
    "            next_state1.append(next_state_viewport)\n",
    "                \n",
    "        current_state = torch.cat([torch.cat(state1,dim=1), torch.cat(next_state1,dim=1)],dim=1) # torch.Size([8, 48, 20]), [8, 48, 20]\n",
    "        # current_state, torch.Size([8, 96, 20])\n",
    "                \n",
    "        ##########################################################################\n",
    "        # 0: left+up   # 1: up         # 2: up+right   #3: left              #4: stop\n",
    "        # 5: right    # 6: down+left   # 7: down       # 8: right + down    \n",
    "\n",
    "        action_list = []\n",
    "        pixel_speed = 1\n",
    "\n",
    "        if self.action_dic[action][0] == \"-\":            \n",
    "            action_list.append(viewport_y-pixel_speed)\n",
    "            action_list.append(viewport_y+12-pixel_speed)\n",
    "        elif self.action_dic[action][0] == \"0\":\n",
    "            action_list.append(viewport_y)\n",
    "            action_list.append(viewport_y+12)\n",
    "        elif self.action_dic[action][0] == \"+\":\n",
    "            action_list.append(viewport_y+pixel_speed)\n",
    "            action_list.append(viewport_y+12+pixel_speed)\n",
    "\n",
    "        if self.action_dic[action][1] == \"-\":\n",
    "            action_list.append(viewport_x-pixel_speed)\n",
    "            action_list.append(viewport_x+20-pixel_speed)\n",
    "        elif self.action_dic[action][1] == \"0\":\n",
    "            action_list.append(viewport_x)\n",
    "            action_list.append(viewport_x+20)\n",
    "        elif self.action_dic[action][1] == \"+\":\n",
    "            action_list.append(viewport_x+pixel_speed)\n",
    "            action_list.append(viewport_x+20+pixel_speed)\n",
    "\n",
    "        next_state2 = []\n",
    "        next_next_state2 = []\n",
    "        \n",
    "        for i in range(4):        \n",
    "            next_state_channel = test_img_array[self.current_step-3+i+4][0:8] # torch.Size([8, 128, 128])\n",
    "            next_state_viewport = next_state_channel.clone()[:,action_list[0]:action_list[1], action_list[2]:action_list[3]] # torch.Size([8, 12, 20])                        \n",
    "            # current_state_viewport torch.Size([8, 12, 20])\n",
    "            \n",
    "            next_next_state_channel = test_img_array[self.current_step+i+1+4][0:8]\n",
    "            next_next_state_viewport = next_next_state_channel.clone()[:,action_list[0]:action_list[1], action_list[2]:action_list[3]]\n",
    "            \n",
    "            next_state2.append(next_state_viewport)\n",
    "            next_next_state2.append(next_next_state_viewport)\n",
    "        \n",
    "        next_state = torch.cat([torch.cat(next_state2,dim=1), torch.cat(next_next_state2,dim=1)],dim=1)\n",
    "\n",
    "        return current_state, next_state, 0\n",
    "\n",
    "    def get_angle(self, center_coordinate1, center_coordinate2):\n",
    "        y1, x1 = center_coordinate1\n",
    "        y2, x2 = center_coordinate2\n",
    "        degrees = math.degrees(math.atan2(y2-y1, x2-x1))\n",
    "            \n",
    "        if degrees < 0: # 0~ 180 ~ 0 ~ -180 -> 0 ~ 360\n",
    "            degrees = 360-abs(degrees)\n",
    "            \n",
    "        return (720- degrees) % 360\n",
    "        \n",
    "    def find_center_coordinate(self, frame):\n",
    "        frame = torch.sum(frame,0)\n",
    "        center_cordinate =  torch.nonzero(frame).sum(0).numpy() / len(torch.nonzero(frame)) # (y,x로 찍혀나옴)\n",
    "        return center_cordinate        \n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        done = 0\n",
    "        reward = 0\n",
    "        angle_reward = 0\n",
    "        distance_reward = 0\n",
    "        \n",
    "   \n",
    "        self.current_step += 1         \n",
    "        self.current_episode_step += 1\n",
    "        \n",
    "        state, next_state, done = self._next_observation()\n",
    "        \n",
    "        if self.current_episode_step == 30:\n",
    "            self.current_episode_step = 0\n",
    "            done = 1\n",
    "\n",
    "        self.action_to_angle = {0:135, 1:90, 2:45, 3: 180, 4:0,\n",
    "            5:0 , 6:225, 7:270, 8:315}\n",
    "        \n",
    "        if done == 0:            \n",
    "            self.pre_frame_center_coor = self.find_center_coordinate(state[:, 24:36])\n",
    "            self.curr_frame_center_coor = self.find_center_coordinate(state[:, 36:48])\n",
    "            angle = self.get_angle(self.pre_frame_center_coor, self.curr_frame_center_coor)\n",
    "            \n",
    "#             print(\"pre_center_coor:\",self.pre_frame_center_coor, \"curr_center_coor:\",self.curr_frame_center_coor)\n",
    "            print(\"frame_angle: \", angle)            \n",
    "            \n",
    "            kk_minus = []\n",
    "            for keys in self.action_to_angle.keys():\n",
    "                kk_minus.append(abs(angle - self.action_to_angle[keys]))\n",
    "            action = np.array(kk_minus).argmin()\n",
    "            \n",
    "            action_idx = action        \n",
    "            self.current_action = action                     \n",
    "            \n",
    "            #  객체가 하나도 안잡히면 angle이 nan이 나옴\n",
    "            if np.isnan(angle):\n",
    "                angle = 0\n",
    "                angle_reward -=2\n",
    "                done = 1\n",
    "            else :\n",
    "            # 우, 우상, 상, 좌상, 좌, 좌하, 하, 우하, 정지 순임        \n",
    "                if angle == 0 and action_idx == 4:\n",
    "                    angle_reward += 0.5\n",
    "                elif self.action_to_angle[action] <= angle+22.5 and self.action_to_angle[action] > angle-22.5:\n",
    "                    angle_reward += 2\n",
    "                elif self.action_to_angle[action] <= angle+45.0 and self.action_to_angle[action] > angle-45.0:\n",
    "                    angle_reward += 1\n",
    "        else:\n",
    "            reward1 = 0\n",
    "            angle_reward = 0\n",
    "            distance_reward = 0\n",
    "       \n",
    "        reward = angle_reward   \n",
    "        \n",
    "        return next_state, reward, done, {}, action\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        #self.current_step = random.randint(4, dataset_len - 50)        \n",
    "        self.current_step = 2358 #random.randint(4, dataset_len - 50)        \n",
    "        #viewport_x = int(predictions[self.current_step][0]['boxes'][0][0])\n",
    "        #viewport_y = int(predictions[self.current_step][0]['boxes'][0][1])\n",
    "        \n",
    "        viewport_x =  21\n",
    "        viewport_y =  25\n",
    "        \n",
    "        state = []\n",
    "        next_state = []\n",
    "        for i in range(4):        \n",
    "            current_state_channel = test_img_array[self.current_step-i][0:8]\n",
    "            current_state_viewport = current_state_channel.clone()[:,viewport_y:viewport_y+12, viewport_x:viewport_x+20]\n",
    "            \n",
    "            next_state_channel = test_img_array[self.current_step-i+4][0:8]\n",
    "            next_state_viewport = next_state_channel.clone()[:,viewport_y:viewport_y+12, viewport_x:viewport_x+20]\n",
    "            \n",
    "            state.append(current_state_viewport)\n",
    "            next_state.append(next_state_viewport)\n",
    "        \n",
    "        current_state = torch.cat([torch.cat(state,dim=1), torch.cat(next_state,dim=1)],dim=1)\n",
    "        \n",
    "        return current_state, self.current_step\n",
    "    \n",
    "    def render(self):\n",
    "        x_axis, y_axis = 1,2                \n",
    "        fig, ax1 = plt.subplots(x_axis, y_axis, figsize=(10, 10))\n",
    "        state, next_state, done  = self._next_observation()\n",
    "                \n",
    "        if done == 1:\n",
    "            return 0 \n",
    "        else:\n",
    "            pre_y, pre_x = self.pre_frame_center_coor \n",
    "            curr_y, curr_x = self.curr_frame_center_coor \n",
    "            \n",
    "            \n",
    "            ax1[0].imshow(state[:,24:36].sum(axis=0).detach().numpy()    , cmap=\"RdYlBu_r\")      # 48 ~ 96 \n",
    "            ax1[0].add_patch(plt.Circle((pre_x,pre_y), 0.5, color='r'))            \n",
    "            ax1[0].set_title(\"state\") #, (\" + self.action_[self.current_action]+ \")\")\n",
    "\n",
    "            \n",
    "            ax1[1].imshow(state[:,36:48].sum(axis=0).detach().numpy()    , cmap=\"RdYlBu_r\")        \n",
    "            ax1[1].add_patch(plt.Circle((curr_x,curr_y), 0.5, color='r'))\n",
    "            ax1[1].set_title(\"state'\")\n",
    "            \n",
    "            for i in ax1:\n",
    "                i.set_xticks(range(0,20,4))\n",
    "                i.set_yticks(range(0,12,4))\n",
    "                i.grid() \n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "            return 0\n",
    "        \n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "action_sequence_list = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "from itertools import count\n",
    "from gym.envs.registration import register\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # exploration\n",
    "GAMMA = 0.9                 # reward discount factor\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 5 #2000\n",
    "\n",
    "N_ACTIONS = 9\n",
    "N_STATES = (16,12,20)\n",
    "\n",
    "env = StarObserverEnv2()\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(8, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(23552, 1)\n",
    "        self.fc2 = nn.Linear(1, 9)\n",
    "        #(64x32 and 2048x32)\n",
    "        #1x2048 and 32x64\n",
    "        \n",
    "    def forward(self, x):      \n",
    "        x = self.conv1(x)      \n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)      \n",
    "        x = self.dropout1(x)      \n",
    "        x = torch.flatten(x, 1)      \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #output = F.log_softmax(x, dim=1)\n",
    "        return x #output\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.eval_net = Net()\n",
    "        self.target_net =  Net()\n",
    "        self.train_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory_counter2 = 0      \n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, 8 , 96*2 , 20))     # initialize memory\n",
    "        self.memory2 = np.zeros((MEMORY_CAPACITY, 2))     # initialize memory\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "\n",
    "    def select_action(self, x):\n",
    "        x = x.float().unsqueeze(0)\n",
    "        if np.random.uniform() < EPSILON:   # Exploration\n",
    "            actions_value = self.eval_net.forward(x) # [[0.3, 0.9]]\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy() # 큰 값의 인덱스를 뽑아냄 [1], (0.9 > 0.3)\n",
    "            action = action[0] # 1\n",
    "        else:\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        return action # 0과 1을 이용\n",
    "\n",
    "dqn = DQN()\n",
    "dqn.eval_net.load_state_dict(torch.load('DQN_Cartpole.pth'))\n",
    "\n",
    "\n",
    "action_sequence_list = []\n",
    "start = []\n",
    "end = []\n",
    "\n",
    "def main():\n",
    "    for i_episode in range(1):\n",
    "        total_reward = 0\n",
    "        state, current_frame = env.reset()\n",
    "        \n",
    "        aa = 0\n",
    "        action2 = 0\n",
    "        for t in range(700):\n",
    "                        \n",
    "            #action = dqn.select_action(state)          \n",
    "            \n",
    "            next_state, reward, done, info, action2 = env.step(action2)\n",
    "            action_sequence_list.append(action2)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done: # 초기 데이터가 모이지 않았을 때\n",
    "                print(\"total_reward\",total_reward)\n",
    "                print(\"end\")\n",
    "                start.append(current_frame)\n",
    "                end.append(current_frame+t)\n",
    "                print(\"current_frame: \",current_frame, \"end_frame: \", current_frame+t)\n",
    "                break\n",
    "            print(\"action: \",action2 ,\"reward: \", reward)\n",
    "            env.render()\n",
    "            state = next_state            \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start, end\n",
    "initial_viewport_x =  21\n",
    "initial_viewport_y =  25\n",
    "action_dic = {0: ['-','-'], 1: ['-','0'],2: ['-','+'],3: ['0','-'],4: ['0', '0'],\n",
    "            5: ['0','+'],6: ['+','-'],7: ['+','0'],8: ['+','+']}\n",
    "action_list1 = []\n",
    "# print(start[0], end[0])\n",
    "# print(end[0]- start[0])\n",
    "# print(len(action_sequence_list))\n",
    "# print(action_sequence_list)\n",
    "# print(len(action_sequence_list))\n",
    "# print(action_sequence_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#viewport_x = int(predictions[start[0]][0]['boxes'][0][0])\n",
    "#viewport_y = int(predictions[start[0]][0]['boxes'][0][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pixel_speed = 1\n",
    "action_list1 = [initial_viewport_x, initial_viewport_y]\n",
    "\n",
    "for action in action_sequence_list:\n",
    "        \n",
    "        if action_dic[action][0] == \"-\":            \n",
    "            action_list1.append(viewport_y-pixel_speed)\n",
    "#            action_list1.append(viewport_y+12-pixel_speed)\n",
    "        elif action_dic[action][0] == \"0\":\n",
    "            action_list1.append(viewport_y)\n",
    "#            action_list1.append(viewport_y+12)\n",
    "        elif action_dic[action][0] == \"+\":\n",
    "            action_list1.append(viewport_y+pixel_speed)\n",
    "#            action_list1.append(viewport_y+12+pixel_speed)\n",
    "\n",
    "        if action_dic[action][1] == \"-\":\n",
    "            action_list1.append(viewport_x-pixel_speed)\n",
    "#            action_list1.append(viewport_x+20-pixel_speed)\n",
    "        elif action_dic[action][1] == \"0\":\n",
    "            action_list1.append(viewport_x)\n",
    "#            action_list1.append(viewport_x+20)\n",
    "        elif action_dic[action][1] == \"+\":\n",
    "            action_list1.append(viewport_x+pixel_speed)\n",
    "#            action_list1.append(viewport_x+20+pixel_speed)\n",
    "        viewport_y, viewport_x = action_list1[-2:]\n",
    "        #print(action_list1)\n",
    "\n",
    "print(action_list1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import patches as patches\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "SCREEN_WIDTH, SCREEN_HEIGHT = 20, 12\n",
    "\n",
    "ims = []\n",
    "\n",
    "# start, end = start[0], end[0]\n",
    "# for i in range(start, end , 1):\n",
    "\n",
    "for i in range(start[0], start[0]+len(action_sequence_list) , 1):\n",
    "    acac1 = test_img_array[i][:1].sum(axis=0).detach().numpy()    \n",
    "    acac1 = np.where(acac1 >= 0.1, 0.12, 0.0)\n",
    "    acac2 = test_img_array[i][1:2].sum(axis=0).detach().numpy()    \n",
    "    acac2 = np.where(acac2 >= 0.1, 0.24, 0.0)\n",
    "    acac3 = test_img_array[i][2:3].sum(axis=0).detach().numpy()    \n",
    "    acac3 = np.where(acac3 >= 0.1, 0.36, 0.0)\n",
    "    acac4 = test_img_array[i][3:4].sum(axis=0).detach().numpy()    \n",
    "    acac4 = np.where(acac4 >= 0.1, 0.48, 0.0)\n",
    "\n",
    "    acac5 = test_img_array[i][4:5].sum(axis=0).detach().numpy()    \n",
    "    acac5 = np.where(acac5 >= 0.1, 0.6, 0.0)\n",
    "    acac6 = test_img_array[i][5:6].sum(axis=0).detach().numpy()    \n",
    "    acac6 = np.where(acac6 >= 0.1, 0.72, 0.0)\n",
    "    acac7 = test_img_array[i][6:7].sum(axis=0).detach().numpy()    \n",
    "    acac7 = np.where(acac7 >= 0.1, 0.84, 0.0)\n",
    "    acac8 = test_img_array[i][7:8].sum(axis=0).detach().numpy()    \n",
    "    acac8 = np.where(acac8 >= 0.1, 0.96, 0.0)\n",
    "\n",
    "    acac = acac1 + acac2 + acac3  + acac4  + acac5  + acac6\n",
    "    acac = np.where(acac >= 1, 0.96, acac)\n",
    "    \n",
    "    print(i)\n",
    "    viewport_y = int(action_list1[(i-start[0])*2+0])\n",
    "    viewport_x = int(action_list1[(i-start[0])*2+1])\n",
    "    print(viewport_y, viewport_x)\n",
    "    \n",
    "    #viewport_x = int(predictions[i][0]['boxes'][0][0])\n",
    "    #viewport_y = int(predictions[i][0]['boxes'][0][1])\n",
    "    \n",
    "    acac[viewport_y:viewport_y+1, viewport_x:viewport_x+20]=1\n",
    "    acac[viewport_y+12:viewport_y+13, viewport_x:viewport_x+21]=1    \n",
    "    acac[viewport_y:viewport_y+12, viewport_x:viewport_x+1]=1\n",
    "    acac[viewport_y:viewport_y+12, viewport_x+20:viewport_x+21]=1        \n",
    "        \n",
    "    im = ax.imshow(acac, cmap=\"RdYlBu_r\")\n",
    "    ttl = plt.text(0.5, 1.01, i, horizontalalignment='center', verticalalignment='bottom', transform=ax.transAxes)\n",
    "    txt = plt.text(i,i,i)\n",
    "    ims.append([im, ttl, txt])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=500)\n",
    "ani.save('Oserver_'+str(start)+'_'+str(end)+'_.gif', writer='imagemagick', fps=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}